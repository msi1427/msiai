{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML Lab 06 Task(combined).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZIhYu1b3B1z"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAUVhMIY3qlS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88b8d32a-498c-4cb6-efd6-a11dd1a50ce3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc8tiADw4SrN"
      },
      "source": [
        "# Task 01 :\n",
        "**What are the different types of Activation Function used in Neural Networks? Briefly explain them.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYp3T6h4jKf"
      },
      "source": [
        "There are four commonly used activation functions.\n",
        "\n",
        "i) **Sigmoid Function:** A mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Its output ranges from 0 to 1. This can be defined by \\\\\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "ii) **Tanh Function:** This is a rescaled logistic sigmoid function. Its output ranges from -1 to 1. This can be defined by \\\\\n",
        "$$g(z) = \\frac{e^{z}+e^{-z}}{e^{z}-e^{-z}}$$\n",
        "\n",
        "iii) **ReLU (Rectifier Linear Unit) Function:** Defined as the positive part of its argument where z is the input to a neuron. This can be defined by \\\\\n",
        "$$g(z) = max(0, z)$$\n",
        "\n",
        "iv) **Leaky ReLU (Rectifier Linear Unit) Function:** Leaky ReLUs are an attempt to fix the *dying ReLU* problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope $\\alpha$ . This can be defined by \\\\\n",
        "$$g(z) = max(\\alpha z, z)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GaNXezOGSok"
      },
      "source": [
        "# Task 02 :\n",
        "**Why are activation funtions important in Neural Network and when do we use them?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpdxtO-GSyJ"
      },
      "source": [
        "Importance of activation functions : \n",
        "\n",
        "i) **Introducing non-linearity :** Non-linear functions make back-propagation possible. Without non-linear function, back-propargation is useless. Back-propagation minimizes the error and enhances the accuracy or efficiency of the model. Moreover, without non-linearity logistic regression with deep neural network is useless. Briefly, it's nothing but logistic regression with extra steps and more time.\n",
        "\n",
        "ii) **Thresholding during classification :** An activation function serves as a threshold, alternatively called classification or a partition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfrqexh_Ir00"
      },
      "source": [
        "# Task 03 :\n",
        "**Given that for a two class problem, w1, w2 = [x1, x2] contains two feature vectors and\n",
        "they are w1 = [(1,0), (0,1), (1,1)] & w2 = [(1,2), (0,3), (2,1)]. Implement the batch weight\n",
        "update for 5 epochs. [Initially assume the weights are 0.1 and learning rate is 0.3].\n",
        "N.B: Implement this without the help of any library.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFy2QgtYXdj7"
      },
      "source": [
        "def sigmoid(z):\n",
        "    s = 1/(1+np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wJDsg4XWct"
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \n",
        "    #forward propagation\n",
        "    m = X.shape[1] #no of samples\n",
        "    A = sigmoid(np.matmul(w.T, X)+ b) #activation\n",
        "    cost = -1/m*np.sum(Y*np.log(A)+ (1-Y)*np.log(1-A)) #cost function\n",
        "\n",
        "    #backward propagation\n",
        "    dw = (1/m)*(np.matmul(X, (A-Y).T)) \n",
        "    db = (1/m)*np.sum(A-Y)\n",
        "\n",
        "    return dw, db, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgxLy8KscLij"
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        dw, db, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        #Weight Update\n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "        print (\"Cost after iteration %i: %f\" %(i+1, cost))\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppqujAl8cQoK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "94796439-6139-4628-9535-f3287f54de33"
      },
      "source": [
        "w = np.array([[.1],[.1]])\n",
        "b = 0.1\n",
        "X = np.array([[1.,0,1.,1.,0,2.],[0,1.,1.,2.,3.,1.]])\n",
        "Y = np.array([[0,0,0,1,1,1]])\n",
        "optimize(w, b, X, Y, num_iterations= 5, learning_rate = 0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 1: 0.664946\n",
            "Cost after iteration 2: 0.650251\n",
            "Cost after iteration 3: 0.638410\n",
            "Cost after iteration 4: 0.628234\n",
            "Cost after iteration 5: 0.619109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmDDr9JeQh-S"
      },
      "source": [
        "#Task 04:\n",
        "**Install Tensorflow and load the Breast Cancer Dataset given to you.\n",
        "N.B: The dataset is already split into training and testing sets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjSOZWHdQqGx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, regularizers\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import normalize, to_categorical\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmngLDkvRGBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "02fc733e-8980-457d-fe81-b0c0e304b319"
      },
      "source": [
        "X_train = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Machine Learning Lab/Lab 6/X_train.xlsx')\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.150365</td>\n",
              "      <td>-0.390642</td>\n",
              "      <td>-1.128550</td>\n",
              "      <td>-0.958764</td>\n",
              "      <td>0.310984</td>\n",
              "      <td>-0.595995</td>\n",
              "      <td>-0.802596</td>\n",
              "      <td>-0.802490</td>\n",
              "      <td>0.294539</td>\n",
              "      <td>0.094251</td>\n",
              "      <td>-0.495052</td>\n",
              "      <td>1.487202</td>\n",
              "      <td>-0.514488</td>\n",
              "      <td>-0.491540</td>\n",
              "      <td>0.281498</td>\n",
              "      <td>-0.604512</td>\n",
              "      <td>-0.469007</td>\n",
              "      <td>-0.611700</td>\n",
              "      <td>0.057982</td>\n",
              "      <td>-0.357637</td>\n",
              "      <td>-1.043176</td>\n",
              "      <td>0.213533</td>\n",
              "      <td>-1.036045</td>\n",
              "      <td>-0.848808</td>\n",
              "      <td>0.342499</td>\n",
              "      <td>-0.730097</td>\n",
              "      <td>-0.812321</td>\n",
              "      <td>-0.757984</td>\n",
              "      <td>-0.016148</td>\n",
              "      <td>-0.385034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.937990</td>\n",
              "      <td>0.680514</td>\n",
              "      <td>-0.948201</td>\n",
              "      <td>-0.821525</td>\n",
              "      <td>-0.609636</td>\n",
              "      <td>-0.909867</td>\n",
              "      <td>-0.660669</td>\n",
              "      <td>-0.898716</td>\n",
              "      <td>0.754935</td>\n",
              "      <td>-0.425471</td>\n",
              "      <td>-0.333818</td>\n",
              "      <td>0.759412</td>\n",
              "      <td>-0.287518</td>\n",
              "      <td>-0.421277</td>\n",
              "      <td>-0.162080</td>\n",
              "      <td>-0.204867</td>\n",
              "      <td>-0.050296</td>\n",
              "      <td>-0.203091</td>\n",
              "      <td>-0.254690</td>\n",
              "      <td>-0.391395</td>\n",
              "      <td>-0.715654</td>\n",
              "      <td>1.066842</td>\n",
              "      <td>-0.689922</td>\n",
              "      <td>-0.668697</td>\n",
              "      <td>-0.095537</td>\n",
              "      <td>-0.537866</td>\n",
              "      <td>-0.375048</td>\n",
              "      <td>-0.606870</td>\n",
              "      <td>0.096690</td>\n",
              "      <td>-0.386158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.574121</td>\n",
              "      <td>-1.033336</td>\n",
              "      <td>0.513941</td>\n",
              "      <td>0.408586</td>\n",
              "      <td>-0.106161</td>\n",
              "      <td>-0.363019</td>\n",
              "      <td>-0.417990</td>\n",
              "      <td>-0.088446</td>\n",
              "      <td>-0.271820</td>\n",
              "      <td>-0.575221</td>\n",
              "      <td>-0.576726</td>\n",
              "      <td>-1.057845</td>\n",
              "      <td>-0.538560</td>\n",
              "      <td>-0.387089</td>\n",
              "      <td>-1.072119</td>\n",
              "      <td>-0.720575</td>\n",
              "      <td>-0.423628</td>\n",
              "      <td>-0.492190</td>\n",
              "      <td>-0.674844</td>\n",
              "      <td>-0.801473</td>\n",
              "      <td>0.297615</td>\n",
              "      <td>-0.977818</td>\n",
              "      <td>0.262137</td>\n",
              "      <td>0.113888</td>\n",
              "      <td>-0.524724</td>\n",
              "      <td>-0.520866</td>\n",
              "      <td>-0.182989</td>\n",
              "      <td>-0.023719</td>\n",
              "      <td>-0.200502</td>\n",
              "      <td>-0.751443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.547220</td>\n",
              "      <td>-0.316022</td>\n",
              "      <td>-0.577622</td>\n",
              "      <td>-0.566615</td>\n",
              "      <td>0.586662</td>\n",
              "      <td>-0.649331</td>\n",
              "      <td>-0.805298</td>\n",
              "      <td>-0.500065</td>\n",
              "      <td>0.331078</td>\n",
              "      <td>0.540567</td>\n",
              "      <td>-0.128226</td>\n",
              "      <td>0.556222</td>\n",
              "      <td>-0.204001</td>\n",
              "      <td>-0.332347</td>\n",
              "      <td>-0.552851</td>\n",
              "      <td>-0.758881</td>\n",
              "      <td>-0.648914</td>\n",
              "      <td>0.601566</td>\n",
              "      <td>0.204548</td>\n",
              "      <td>-0.115963</td>\n",
              "      <td>-0.701325</td>\n",
              "      <td>-0.757927</td>\n",
              "      <td>-0.735737</td>\n",
              "      <td>-0.658966</td>\n",
              "      <td>-0.816748</td>\n",
              "      <td>-1.034921</td>\n",
              "      <td>-1.091633</td>\n",
              "      <td>-0.852545</td>\n",
              "      <td>-1.076186</td>\n",
              "      <td>-0.546883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.527398</td>\n",
              "      <td>0.791240</td>\n",
              "      <td>-0.561563</td>\n",
              "      <td>-0.523571</td>\n",
              "      <td>-1.051446</td>\n",
              "      <td>-1.017532</td>\n",
              "      <td>-0.905149</td>\n",
              "      <td>-0.935806</td>\n",
              "      <td>-0.969721</td>\n",
              "      <td>-0.426939</td>\n",
              "      <td>-0.628828</td>\n",
              "      <td>-0.130929</td>\n",
              "      <td>-0.613234</td>\n",
              "      <td>-0.466581</td>\n",
              "      <td>-0.671490</td>\n",
              "      <td>-0.744016</td>\n",
              "      <td>-0.710063</td>\n",
              "      <td>-1.204498</td>\n",
              "      <td>-0.542935</td>\n",
              "      <td>-0.503025</td>\n",
              "      <td>-0.427026</td>\n",
              "      <td>1.058637</td>\n",
              "      <td>-0.422423</td>\n",
              "      <td>-0.440955</td>\n",
              "      <td>-0.303494</td>\n",
              "      <td>-0.467251</td>\n",
              "      <td>-0.724565</td>\n",
              "      <td>-0.783118</td>\n",
              "      <td>0.311240</td>\n",
              "      <td>-0.082129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>1.330176</td>\n",
              "      <td>0.199096</td>\n",
              "      <td>1.197454</td>\n",
              "      <td>1.252698</td>\n",
              "      <td>-0.521129</td>\n",
              "      <td>-0.881910</td>\n",
              "      <td>-0.093733</td>\n",
              "      <td>0.252625</td>\n",
              "      <td>-0.947798</td>\n",
              "      <td>-1.865718</td>\n",
              "      <td>-0.261297</td>\n",
              "      <td>-0.705403</td>\n",
              "      <td>-0.227091</td>\n",
              "      <td>-0.074366</td>\n",
              "      <td>0.199824</td>\n",
              "      <td>-0.796044</td>\n",
              "      <td>-0.168733</td>\n",
              "      <td>0.506501</td>\n",
              "      <td>-0.149652</td>\n",
              "      <td>-0.780374</td>\n",
              "      <td>0.758192</td>\n",
              "      <td>-0.203276</td>\n",
              "      <td>0.649345</td>\n",
              "      <td>0.605735</td>\n",
              "      <td>-0.361014</td>\n",
              "      <td>-0.888328</td>\n",
              "      <td>-0.233135</td>\n",
              "      <td>0.229158</td>\n",
              "      <td>-0.535836</td>\n",
              "      <td>-1.467962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>-0.467933</td>\n",
              "      <td>1.176375</td>\n",
              "      <td>-0.323157</td>\n",
              "      <td>-0.497297</td>\n",
              "      <td>1.609573</td>\n",
              "      <td>2.702147</td>\n",
              "      <td>1.799057</td>\n",
              "      <td>0.952923</td>\n",
              "      <td>0.806090</td>\n",
              "      <td>2.901339</td>\n",
              "      <td>-0.369374</td>\n",
              "      <td>0.718774</td>\n",
              "      <td>-0.386265</td>\n",
              "      <td>-0.342834</td>\n",
              "      <td>0.051260</td>\n",
              "      <td>2.693275</td>\n",
              "      <td>1.479076</td>\n",
              "      <td>0.453876</td>\n",
              "      <td>-0.321866</td>\n",
              "      <td>2.443477</td>\n",
              "      <td>-0.238701</td>\n",
              "      <td>2.484648</td>\n",
              "      <td>-0.277294</td>\n",
              "      <td>-0.289867</td>\n",
              "      <td>2.337996</td>\n",
              "      <td>5.270909</td>\n",
              "      <td>4.199765</td>\n",
              "      <td>1.633011</td>\n",
              "      <td>2.323247</td>\n",
              "      <td>6.968987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>-1.324229</td>\n",
              "      <td>-0.200482</td>\n",
              "      <td>-1.317546</td>\n",
              "      <td>-1.048765</td>\n",
              "      <td>0.325493</td>\n",
              "      <td>-0.867634</td>\n",
              "      <td>-0.776990</td>\n",
              "      <td>-0.898197</td>\n",
              "      <td>-1.108571</td>\n",
              "      <td>1.016245</td>\n",
              "      <td>0.370968</td>\n",
              "      <td>0.068566</td>\n",
              "      <td>0.217023</td>\n",
              "      <td>-0.205664</td>\n",
              "      <td>-0.058930</td>\n",
              "      <td>-0.919426</td>\n",
              "      <td>-0.257238</td>\n",
              "      <td>-0.862603</td>\n",
              "      <td>-0.134995</td>\n",
              "      <td>-0.382572</td>\n",
              "      <td>-0.867133</td>\n",
              "      <td>-0.085125</td>\n",
              "      <td>-0.923429</td>\n",
              "      <td>-0.753887</td>\n",
              "      <td>0.037201</td>\n",
              "      <td>-0.960905</td>\n",
              "      <td>-0.767691</td>\n",
              "      <td>-0.979750</td>\n",
              "      <td>-0.715423</td>\n",
              "      <td>-0.119781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>-1.243810</td>\n",
              "      <td>-0.224553</td>\n",
              "      <td>-1.280076</td>\n",
              "      <td>-1.022212</td>\n",
              "      <td>-1.952479</td>\n",
              "      <td>-1.583811</td>\n",
              "      <td>-1.125696</td>\n",
              "      <td>-1.262871</td>\n",
              "      <td>-0.571443</td>\n",
              "      <td>0.264556</td>\n",
              "      <td>-0.171175</td>\n",
              "      <td>6.788612</td>\n",
              "      <td>-0.292431</td>\n",
              "      <td>-0.390026</td>\n",
              "      <td>-1.862462</td>\n",
              "      <td>-1.047838</td>\n",
              "      <td>-1.012913</td>\n",
              "      <td>-1.977069</td>\n",
              "      <td>2.133101</td>\n",
              "      <td>-0.775771</td>\n",
              "      <td>-1.287179</td>\n",
              "      <td>-0.772695</td>\n",
              "      <td>-1.323643</td>\n",
              "      <td>-0.985726</td>\n",
              "      <td>-2.711807</td>\n",
              "      <td>-1.468356</td>\n",
              "      <td>-1.341360</td>\n",
              "      <td>-1.754014</td>\n",
              "      <td>-1.581571</td>\n",
              "      <td>-1.006018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>-0.736941</td>\n",
              "      <td>1.149897</td>\n",
              "      <td>-0.712266</td>\n",
              "      <td>-0.699660</td>\n",
              "      <td>-0.275920</td>\n",
              "      <td>-0.024163</td>\n",
              "      <td>0.305156</td>\n",
              "      <td>-0.198159</td>\n",
              "      <td>-1.539735</td>\n",
              "      <td>0.445137</td>\n",
              "      <td>-0.576374</td>\n",
              "      <td>3.129346</td>\n",
              "      <td>-0.436866</td>\n",
              "      <td>-0.489023</td>\n",
              "      <td>0.421261</td>\n",
              "      <td>0.271963</td>\n",
              "      <td>0.833791</td>\n",
              "      <td>0.173774</td>\n",
              "      <td>-0.689500</td>\n",
              "      <td>0.394237</td>\n",
              "      <td>-0.772970</td>\n",
              "      <td>1.907023</td>\n",
              "      <td>-0.731599</td>\n",
              "      <td>-0.694817</td>\n",
              "      <td>-0.117660</td>\n",
              "      <td>-0.001058</td>\n",
              "      <td>0.478937</td>\n",
              "      <td>-0.274605</td>\n",
              "      <td>-1.258951</td>\n",
              "      <td>0.215157</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>455 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           x1        x2        x3  ...       x28       x29       x30\n",
              "0   -1.150365 -0.390642 -1.128550  ... -0.757984 -0.016148 -0.385034\n",
              "1   -0.937990  0.680514 -0.948201  ... -0.606870  0.096690 -0.386158\n",
              "2    0.574121 -1.033336  0.513941  ... -0.023719 -0.200502 -0.751443\n",
              "3   -0.547220 -0.316022 -0.577622  ... -0.852545 -1.076186 -0.546883\n",
              "4   -0.527398  0.791240 -0.561563  ... -0.783118  0.311240 -0.082129\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "450  1.330176  0.199096  1.197454  ...  0.229158 -0.535836 -1.467962\n",
              "451 -0.467933  1.176375 -0.323157  ...  1.633011  2.323247  6.968987\n",
              "452 -1.324229 -0.200482 -1.317546  ... -0.979750 -0.715423 -0.119781\n",
              "453 -1.243810 -0.224553 -1.280076  ... -1.754014 -1.581571 -1.006018\n",
              "454 -0.736941  1.149897 -0.712266  ... -0.274605 -1.258951  0.215157\n",
              "\n",
              "[455 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI05OIBER6iO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "d592e816-a9e3-46b3-bc58-d69839ca7a5f"
      },
      "source": [
        "X_test = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Machine Learning Lab/Lab 6/X_test.xlsx')\n",
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.201756</td>\n",
              "      <td>0.329079</td>\n",
              "      <td>-0.130868</td>\n",
              "      <td>-0.271455</td>\n",
              "      <td>1.029198</td>\n",
              "      <td>0.864118</td>\n",
              "      <td>0.733639</td>\n",
              "      <td>0.856697</td>\n",
              "      <td>1.120328</td>\n",
              "      <td>1.553585</td>\n",
              "      <td>-0.041976</td>\n",
              "      <td>-0.515882</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-0.138756</td>\n",
              "      <td>-0.559540</td>\n",
              "      <td>-0.137974</td>\n",
              "      <td>0.098071</td>\n",
              "      <td>0.287512</td>\n",
              "      <td>-0.424461</td>\n",
              "      <td>0.113051</td>\n",
              "      <td>0.031504</td>\n",
              "      <td>0.676289</td>\n",
              "      <td>0.185286</td>\n",
              "      <td>-0.062808</td>\n",
              "      <td>1.103531</td>\n",
              "      <td>0.874443</td>\n",
              "      <td>1.219091</td>\n",
              "      <td>1.389329</td>\n",
              "      <td>1.082033</td>\n",
              "      <td>1.540297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.255558</td>\n",
              "      <td>1.467633</td>\n",
              "      <td>-0.317804</td>\n",
              "      <td>-0.324002</td>\n",
              "      <td>-0.616891</td>\n",
              "      <td>-1.016540</td>\n",
              "      <td>-0.769012</td>\n",
              "      <td>-0.726495</td>\n",
              "      <td>-0.695677</td>\n",
              "      <td>-1.002451</td>\n",
              "      <td>-0.683394</td>\n",
              "      <td>0.258826</td>\n",
              "      <td>-0.742440</td>\n",
              "      <td>-0.476229</td>\n",
              "      <td>-0.434915</td>\n",
              "      <td>-0.970882</td>\n",
              "      <td>-0.526938</td>\n",
              "      <td>-0.881956</td>\n",
              "      <td>-0.861714</td>\n",
              "      <td>-0.722066</td>\n",
              "      <td>-0.390180</td>\n",
              "      <td>1.426216</td>\n",
              "      <td>-0.465282</td>\n",
              "      <td>-0.423883</td>\n",
              "      <td>-0.157482</td>\n",
              "      <td>-0.951752</td>\n",
              "      <td>-0.644332</td>\n",
              "      <td>-0.833694</td>\n",
              "      <td>-0.731316</td>\n",
              "      <td>-0.877325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.026193</td>\n",
              "      <td>-0.840768</td>\n",
              "      <td>-0.091751</td>\n",
              "      <td>-0.132260</td>\n",
              "      <td>-1.215402</td>\n",
              "      <td>-0.941988</td>\n",
              "      <td>-0.857154</td>\n",
              "      <td>-0.575023</td>\n",
              "      <td>-0.805295</td>\n",
              "      <td>-0.999514</td>\n",
              "      <td>-0.662976</td>\n",
              "      <td>-1.025150</td>\n",
              "      <td>-0.598987</td>\n",
              "      <td>-0.441202</td>\n",
              "      <td>-0.762670</td>\n",
              "      <td>-0.904789</td>\n",
              "      <td>-0.716404</td>\n",
              "      <td>-0.150466</td>\n",
              "      <td>-1.076677</td>\n",
              "      <td>-0.616190</td>\n",
              "      <td>-0.275547</td>\n",
              "      <td>-1.022124</td>\n",
              "      <td>-0.310399</td>\n",
              "      <td>-0.328279</td>\n",
              "      <td>-1.285756</td>\n",
              "      <td>-0.981828</td>\n",
              "      <td>-1.027447</td>\n",
              "      <td>-0.494838</td>\n",
              "      <td>-1.220809</td>\n",
              "      <td>-0.921159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.041767</td>\n",
              "      <td>-0.236588</td>\n",
              "      <td>-0.024635</td>\n",
              "      <td>-0.057911</td>\n",
              "      <td>-2.223804</td>\n",
              "      <td>-1.013566</td>\n",
              "      <td>-0.807228</td>\n",
              "      <td>-0.906497</td>\n",
              "      <td>-0.637214</td>\n",
              "      <td>-0.999514</td>\n",
              "      <td>-0.607353</td>\n",
              "      <td>-0.999105</td>\n",
              "      <td>-0.571967</td>\n",
              "      <td>-0.413726</td>\n",
              "      <td>-1.349883</td>\n",
              "      <td>-0.645677</td>\n",
              "      <td>-0.665650</td>\n",
              "      <td>-1.087024</td>\n",
              "      <td>-1.159730</td>\n",
              "      <td>-0.672580</td>\n",
              "      <td>-0.007389</td>\n",
              "      <td>-0.045742</td>\n",
              "      <td>-0.036398</td>\n",
              "      <td>-0.104976</td>\n",
              "      <td>-1.681759</td>\n",
              "      <td>-0.229905</td>\n",
              "      <td>-0.556576</td>\n",
              "      <td>-0.599974</td>\n",
              "      <td>-0.426177</td>\n",
              "      <td>-0.378290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.306528</td>\n",
              "      <td>-0.176411</td>\n",
              "      <td>-0.379156</td>\n",
              "      <td>-0.363413</td>\n",
              "      <td>-0.477601</td>\n",
              "      <td>-1.301861</td>\n",
              "      <td>-0.796034</td>\n",
              "      <td>-0.504734</td>\n",
              "      <td>-1.251074</td>\n",
              "      <td>-0.592839</td>\n",
              "      <td>-0.769644</td>\n",
              "      <td>2.091230</td>\n",
              "      <td>-0.813184</td>\n",
              "      <td>-0.547960</td>\n",
              "      <td>-0.933413</td>\n",
              "      <td>-1.152866</td>\n",
              "      <td>-0.580685</td>\n",
              "      <td>-0.001078</td>\n",
              "      <td>0.755388</td>\n",
              "      <td>-0.741630</td>\n",
              "      <td>-0.605116</td>\n",
              "      <td>-0.447782</td>\n",
              "      <td>-0.667162</td>\n",
              "      <td>-0.572410</td>\n",
              "      <td>-1.568489</td>\n",
              "      <td>-1.344779</td>\n",
              "      <td>-1.099005</td>\n",
              "      <td>-0.985727</td>\n",
              "      <td>-1.457609</td>\n",
              "      <td>-1.225189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.936575</td>\n",
              "      <td>1.542253</td>\n",
              "      <td>0.933931</td>\n",
              "      <td>0.822257</td>\n",
              "      <td>0.303729</td>\n",
              "      <td>0.223683</td>\n",
              "      <td>1.038596</td>\n",
              "      <td>0.448190</td>\n",
              "      <td>-1.832050</td>\n",
              "      <td>-0.588435</td>\n",
              "      <td>0.447361</td>\n",
              "      <td>0.844382</td>\n",
              "      <td>0.462661</td>\n",
              "      <td>0.382659</td>\n",
              "      <td>8.493677</td>\n",
              "      <td>3.458261</td>\n",
              "      <td>3.615112</td>\n",
              "      <td>4.689356</td>\n",
              "      <td>0.149586</td>\n",
              "      <td>3.394828</td>\n",
              "      <td>0.371308</td>\n",
              "      <td>0.415373</td>\n",
              "      <td>0.395147</td>\n",
              "      <td>0.238685</td>\n",
              "      <td>-0.361014</td>\n",
              "      <td>-0.474443</td>\n",
              "      <td>0.064230</td>\n",
              "      <td>-0.069697</td>\n",
              "      <td>-2.067886</td>\n",
              "      <td>-0.860466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>-0.385814</td>\n",
              "      <td>-0.590431</td>\n",
              "      <td>-0.383273</td>\n",
              "      <td>-0.447265</td>\n",
              "      <td>1.167037</td>\n",
              "      <td>0.166183</td>\n",
              "      <td>-0.626442</td>\n",
              "      <td>-0.486059</td>\n",
              "      <td>1.134943</td>\n",
              "      <td>0.522949</td>\n",
              "      <td>-0.067323</td>\n",
              "      <td>-0.382146</td>\n",
              "      <td>-0.162243</td>\n",
              "      <td>-0.244675</td>\n",
              "      <td>0.177997</td>\n",
              "      <td>-0.540477</td>\n",
              "      <td>-0.376640</td>\n",
              "      <td>-0.415459</td>\n",
              "      <td>-0.302324</td>\n",
              "      <td>-0.031186</td>\n",
              "      <td>-0.369710</td>\n",
              "      <td>-0.623366</td>\n",
              "      <td>-0.396117</td>\n",
              "      <td>-0.438907</td>\n",
              "      <td>0.665495</td>\n",
              "      <td>-0.352828</td>\n",
              "      <td>-0.627784</td>\n",
              "      <td>-0.521045</td>\n",
              "      <td>0.265152</td>\n",
              "      <td>0.116811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>1.718115</td>\n",
              "      <td>0.093184</td>\n",
              "      <td>1.728619</td>\n",
              "      <td>1.666369</td>\n",
              "      <td>1.225074</td>\n",
              "      <td>0.903774</td>\n",
              "      <td>1.618914</td>\n",
              "      <td>2.002591</td>\n",
              "      <td>-0.312014</td>\n",
              "      <td>-0.313892</td>\n",
              "      <td>0.107642</td>\n",
              "      <td>-0.385840</td>\n",
              "      <td>0.089782</td>\n",
              "      <td>0.255975</td>\n",
              "      <td>0.733526</td>\n",
              "      <td>0.119309</td>\n",
              "      <td>0.771999</td>\n",
              "      <td>1.265322</td>\n",
              "      <td>0.486686</td>\n",
              "      <td>0.113051</td>\n",
              "      <td>1.181923</td>\n",
              "      <td>-0.076920</td>\n",
              "      <td>1.151828</td>\n",
              "      <td>1.020588</td>\n",
              "      <td>1.506170</td>\n",
              "      <td>0.276827</td>\n",
              "      <td>1.320386</td>\n",
              "      <td>1.576305</td>\n",
              "      <td>0.203171</td>\n",
              "      <td>-0.154062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>1.188593</td>\n",
              "      <td>0.343521</td>\n",
              "      <td>1.193337</td>\n",
              "      <td>1.112945</td>\n",
              "      <td>0.753520</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.893194</td>\n",
              "      <td>1.188431</td>\n",
              "      <td>0.184921</td>\n",
              "      <td>-0.478324</td>\n",
              "      <td>0.501223</td>\n",
              "      <td>-0.530844</td>\n",
              "      <td>0.192950</td>\n",
              "      <td>0.567860</td>\n",
              "      <td>-0.291281</td>\n",
              "      <td>-0.359808</td>\n",
              "      <td>-0.075400</td>\n",
              "      <td>-0.201393</td>\n",
              "      <td>-0.602782</td>\n",
              "      <td>-0.377968</td>\n",
              "      <td>1.147124</td>\n",
              "      <td>0.108510</td>\n",
              "      <td>1.039508</td>\n",
              "      <td>1.044489</td>\n",
              "      <td>0.740713</td>\n",
              "      <td>0.011365</td>\n",
              "      <td>0.543625</td>\n",
              "      <td>0.560198</td>\n",
              "      <td>0.269920</td>\n",
              "      <td>-0.273201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0.262638</td>\n",
              "      <td>-0.580802</td>\n",
              "      <td>0.284593</td>\n",
              "      <td>0.099451</td>\n",
              "      <td>0.175321</td>\n",
              "      <td>0.655927</td>\n",
              "      <td>-0.132206</td>\n",
              "      <td>0.330177</td>\n",
              "      <td>-0.516634</td>\n",
              "      <td>0.884112</td>\n",
              "      <td>-0.107455</td>\n",
              "      <td>-0.678988</td>\n",
              "      <td>-0.256076</td>\n",
              "      <td>-0.114217</td>\n",
              "      <td>-1.014031</td>\n",
              "      <td>-0.392969</td>\n",
              "      <td>-0.370525</td>\n",
              "      <td>-0.272692</td>\n",
              "      <td>-1.218356</td>\n",
              "      <td>-0.181560</td>\n",
              "      <td>0.103149</td>\n",
              "      <td>-0.838335</td>\n",
              "      <td>0.078878</td>\n",
              "      <td>-0.041468</td>\n",
              "      <td>-0.838871</td>\n",
              "      <td>-0.224020</td>\n",
              "      <td>-0.410652</td>\n",
              "      <td>-0.193837</td>\n",
              "      <td>-1.155649</td>\n",
              "      <td>0.112315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           x1        x2        x3  ...       x28       x29       x30\n",
              "0   -0.201756  0.329079 -0.130868  ...  1.389329  1.082033  1.540297\n",
              "1   -0.255558  1.467633 -0.317804  ... -0.833694 -0.731316 -0.877325\n",
              "2   -0.026193 -0.840768 -0.091751  ... -0.494838 -1.220809 -0.921159\n",
              "3    0.041767 -0.236588 -0.024635  ... -0.599974 -0.426177 -0.378290\n",
              "4   -0.306528 -0.176411 -0.379156  ... -0.985727 -1.457609 -1.225189\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "109  0.936575  1.542253  0.933931  ... -0.069697 -2.067886 -0.860466\n",
              "110 -0.385814 -0.590431 -0.383273  ... -0.521045  0.265152  0.116811\n",
              "111  1.718115  0.093184  1.728619  ...  1.576305  0.203171 -0.154062\n",
              "112  1.188593  0.343521  1.193337  ...  0.560198  0.269920 -0.273201\n",
              "113  0.262638 -0.580802  0.284593  ... -0.193837 -1.155649  0.112315\n",
              "\n",
              "[114 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AefBsVWR-f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "029e085b-65a6-4459-c25c-4b4438f8fa09"
      },
      "source": [
        "Y_train = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Machine Learning Lab/Lab 6/Y_train.xlsx')\n",
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y_train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>455 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Y_train\n",
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "..       ...\n",
              "450        1\n",
              "451        1\n",
              "452        0\n",
              "453        0\n",
              "454        0\n",
              "\n",
              "[455 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWVLm2HRSCoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "afdc4ee1-3c41-4e7d-8e83-31727f1e8810"
      },
      "source": [
        "Y_test = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Machine Learning Lab/Lab 6/Y_test.xlsx')\n",
        "Y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Y_test\n",
              "0         1\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "..      ...\n",
              "109       1\n",
              "110       0\n",
              "111       1\n",
              "112       1\n",
              "113       0\n",
              "\n",
              "[114 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePbTYosSLX5"
      },
      "source": [
        "#Task 05:\n",
        "**Build your Neural Network model by importing the Sequential Model from Keras library.\n",
        "Note: Since Breast Cancer is a 30 feature dataset use Relu activation function for your\n",
        "input & hidden layers and Sigmoid activation function for the output layer.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CffNhep5UeXz"
      },
      "source": [
        "X_train = normalize(X_train)\n",
        "X_test = normalize(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azIb0wauSa04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "297b9006-90ce-4b64-9d4a-0b6ebf53fd67"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  keras.layers.Dense(64,input_shape=(X_train.shape[1],),name='input_layer'),\n",
        "  keras.layers.Activation(tf.nn.relu,name='relu_1'),\n",
        "  keras.layers.Dense(128,name='hidden_layer_1'),\n",
        "  keras.layers.Activation(tf.nn.relu,name='relu_2'),\n",
        "  keras.layers.Dense(64,name='hidden_layer_2'),\n",
        "  keras.layers.Activation(tf.nn.relu,name='relu_3'),\n",
        "  keras.layers.Dense(32,name='hidden_layer_3'),\n",
        "  keras.layers.Activation(tf.nn.relu,name='relu_4'),\n",
        "  keras.layers.Dense(16,name='hidden_layer_4'),\n",
        "  keras.layers.Activation(tf.nn.relu,name='relu_5'),\n",
        "  keras.layers.Dense(1,name='output_layer'),\n",
        "  keras.layers.Activation(tf.nn.sigmoid,name='sigmoid'),\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (Dense)          (None, 64)                1984      \n",
            "_________________________________________________________________\n",
            "relu_1 (Activation)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_1 (Dense)       (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "relu_2 (Activation)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "relu_3 (Activation)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "relu_4 (Activation)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "relu_5 (Activation)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "sigmoid (Activation)         (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 21,185\n",
            "Trainable params: 21,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9qDVyFKUhdg"
      },
      "source": [
        "#Task 06:\n",
        "**Calculate the loss function and optimizer for the Network.\n",
        "Hint: Use classifier.compile() to perform this. Since this is a binary classification problem\n",
        "for the ’loss’ parameter use ’binary_crossentropy’.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftQ7_Rm7UtUw"
      },
      "source": [
        "adam =  keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_4tLX6IWBzx"
      },
      "source": [
        "#Task 07:\n",
        "**Train your Neural Network model using the fit function and set the parameter ’epochs’ as\n",
        "100 and ’batch_size’ as 1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BEavB60WJ9g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "476da916-af09-4ba5-9002-1d27fe2c5691"
      },
      "source": [
        "history = model.fit(X_train, Y_train,batch_size=1, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.1761 - accuracy: 0.9495\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0771 - accuracy: 0.9758\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0664 - accuracy: 0.9780\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0453 - accuracy: 0.9890\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0605 - accuracy: 0.9846\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0443 - accuracy: 0.9846\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0281 - accuracy: 0.9868\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0381 - accuracy: 0.9846\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0217 - accuracy: 0.9868\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0233 - accuracy: 0.9912\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0155 - accuracy: 0.9934\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9956\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0166 - accuracy: 0.9934\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9912\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9934\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.1319 - accuracy: 0.9890\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0395 - accuracy: 0.9978\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 9.3589e-04 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.6554e-04 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.9658e-04 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 9.0598e-05 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.6171e-05 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.5683e-05 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.6479e-05 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.0797e-05 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 7.3035e-06 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 5.3586e-06 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.8914e-06 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.8306e-06 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.0680e-06 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.6117e-06 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.1936e-06 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 9.2428e-07 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 7.3774e-07 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 5.5076e-07 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.0737e-07 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.0401e-07 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.6000e-07 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.8721e-07 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.6769e-07 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.1441e-07 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 8.7218e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 6.7385e-08 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 5.1393e-08 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.5219e-08 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.2131e-08 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.5236e-08 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.0233e-08 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.6117e-08 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.1968e-08 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.0870e-08 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 7.8049e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 6.2342e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 5.2583e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.8739e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.0770e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.4759e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.1039e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.7325e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.3584e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.2421e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.0314e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 8.2263e-10 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 7.4490e-10 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 6.3952e-10 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.7807e-10 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.3004e-10 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.6263e-10 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.8136e-10 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.5407e-10 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.2099e-10 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.8354e-10 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.3666e-10 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.0601e-10 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.5724e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.5329e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.4436e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.7285e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 8.1378e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2327 - accuracy: 0.9868\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.1063 - accuracy: 0.9846\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0181 - accuracy: 0.9890\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 8.5515e-04 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.8564e-04 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.0499e-04 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.9703e-04 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.3302e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 9.1246e-05 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 6.3878e-05 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 4.4931e-05 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 3.1802e-05 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2.3177e-05 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.6831e-05 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 1.2425e-05 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 9.1919e-06 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 6.7421e-06 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 5.0773e-06 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKQaSaM4WV7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "f1fd6e6e-c6bc-4ddd-8d47-88956a380649"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTTam-dWXgQ5"
      },
      "source": [
        "#Task 08:\n",
        "**Evaluate your Neural Network to check whether the model is best fit for the given problem\n",
        "and corresponding data. Print the test accuracy and test loss.\n",
        "Hint: Use classifier.evaluate() function to implement this.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA4G90ZJXnBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a769da8b-33e5-4999-d7d4-36fb0954ce10"
      },
      "source": [
        "preds = model.evaluate(X_test, Y_test,batch_size=1)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]*100.0) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114/114 [==============================] - 0s 926us/step - loss: 0.2689 - accuracy: 0.9649\n",
            "Loss = 0.26888787746429443\n",
            "Test Accuracy = 96.49122953414917%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsDUV8HZX7HA"
      },
      "source": [
        "#Task 09:\n",
        "**Predict your model using the predict() function and print the correctly & incorrectly\n",
        "classified samples out of the total test samples.\n",
        "Hint: Since you used a sigmoid function to get your output, the predicted values are\n",
        "between 0 & 1. Thus in order to make them binary use 0.5 as a threshold. For instance,if\n",
        "predicted output >= 0.5, y_pred = 1 else y_pred = 0.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9id3k7JYG5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c7d58486-6e0b-4f02-f88e-b4a920458916"
      },
      "source": [
        "y_pred = model.predict(X_test, batch_size=1)\n",
        "y_pred = y_pred >= 0.5 \n",
        "y_pred = [(1 if True in x else 0) for x in y_pred]\n",
        "y_pred = np.asarray(y_pred)\n",
        "Y_test = np.asarray(Y_test)\n",
        "total = Y_test.shape[0]\n",
        "print(\"Total test Samples: \",total)\n",
        "correct = 0\n",
        "for i in range(y_pred.shape[0]):\n",
        "  if y_pred[i] == Y_test[i] :\n",
        "    correct = correct + 1\n",
        "print(\"Correctly detected: \",correct)\n",
        "print(\"Incorrectly detected = \",total-correct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total test Samples:  114\n",
            "Correctly detected:  110\n",
            "Incorrectly detected =  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gT4sp0IgGTk"
      },
      "source": [
        "#Task 10:\n",
        "**Show the Confusion Matrix and visualize it with the help of seaborn library.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFOXiC2IgJQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5f08a3bd-3838-4baa-b86f-b2cff1c85828"
      },
      "source": [
        "mat = confusion_matrix(y_pred, Y_test)\n",
        "heat = sns.heatmap(mat,square=True,annot=True,fmt='d',cbar=True,cmap=plt.cm.gist_heat)\n",
        "class_label=['Has No Cancer','Has Cancer']\n",
        "heat.set_xticklabels(class_label)\n",
        "heat.set_yticklabels(class_label)\n",
        "heat.set_xlabel('Predicted Label')\n",
        "heat.set_ylabel('True Label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(83.40000000000006, 0.5, 'True Label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}